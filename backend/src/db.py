"""
–í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î Qdrant –∏–∑ JSONL –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤.
–ú–æ–¥–µ–ª—å: BAAI/bge-m3 (8192 —Ç–æ–∫–µ–Ω–æ–≤, –ª—É—á—à–µ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
–£—Å–∫–æ—Ä–µ–Ω–∏–µ: MPS (Apple Silicon) ‚Üí –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏

–£—Å—Ç–∞–Ω–æ–≤–∫–∞:
    pip install qdrant-client sentence-transformers tqdm torch

–ó–∞–ø—É—Å–∫ Qdrant:
    docker run -p 6333:6333 qdrant/qdrant

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python setup_qdrant.py --input protocols.jsonl              # –≤—Å—ë —Å—Ä–∞–∑—É
    python setup_qdrant.py --input protocols.jsonl --encode-only
    python setup_qdrant.py --input protocols.jsonl --upload-only
    python setup_qdrant.py --input protocols.jsonl --query "–±–æ–ª—å –≤ –∂–∏–≤–æ—Ç–µ –∂–µ–ª—Ç—É—Ö–∞"
"""

import json
import re
import argparse
import hashlib
from pathlib import Path

import torch
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, OptimizersConfigDiff
from qdrant_client.models import QueryRequest

# ‚îÄ‚îÄ‚îÄ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

COLLECTION_NAME = "medical_protocols_v5"
EMBEDDING_MODEL  = "BAAI/bge-m3"
VECTOR_SIZE      = 1024

# bge-m3 –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–æ 8192 —Ç–æ–∫–µ–Ω–æ–≤, –±–µ—Ä—ë–º ~3000 —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ —á–∞–Ω–∫ (~900 —Ç–æ–∫–µ–Ω–æ–≤)
MAX_CHARS_PER_CHUNK = 5000
BATCH_SIZE          = 16   # bge-m3 —Ç—è–∂–µ–ª–µ–µ —á–µ–º e5-base

QDRANT_URL  = "http://localhost:6333"
CACHE_FILE  = "points_cache.jsonl"


# ‚îÄ‚îÄ‚îÄ –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ (MPS / CUDA / CPU) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def get_device() -> str:
    if torch.backends.mps.is_available():
        print("üçé –ò—Å–ø–æ–ª—å–∑—É–µ–º MPS (Apple Silicon GPU)")
        return "mps"
    if torch.cuda.is_available():
        print("‚ö° –ò—Å–ø–æ–ª—å–∑—É–µ–º CUDA GPU")
        return "cuda"
    print("üíª –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU")
    return "cpu"


# ‚îÄ‚îÄ‚îÄ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏—Ö —Å–µ–∫—Ü–∏–π ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

_SECTION_PATTERNS = {
    "symptoms": re.compile(
        r"(?:–∂–∞–ª–æ–±[—ã–∏]|–ñ–ê–õ–û–ë[–´–ò]|–∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ\s+–∫—Ä–∏—Ç–µ—Ä–∏–∏|–ö–õ–ò–ù–ò–ß–ï–°–ö–ò–ï\s+–ö–†–ò–¢–ï–†–ò–ò"
        r"|–∫—Ä–∏—Ç–µ—Ä–∏–∏\s+–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏|–ö–†–ò–¢–ï–†–ò–ò\s+–î–ò–ê–ì–ù–û–°–¢–ò–ö–ò"
        r"|–∫–ª–∏–Ω–∏—á–µ—Å–∫–∞—è\s+–∫–∞—Ä—Ç–∏–Ω–∞|–ö–õ–ò–ù–ò–ß–ï–°–ö–ê–Ø\s+–ö–ê–†–¢–ò–ù–ê)(.*?)"
        r"(?=\n\s*\d+\.\d+\s|\nI{1,3}V?\b|\nVII?\b|\Z)",
        re.IGNORECASE | re.DOTALL,
    ),
    "diagnosis": re.compile(
        r"(?:–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫[–∞–∏]|–î–ò–ê–ì–ù–û–°–¢–ò–ö[–ê–ò]|–ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω|–õ–ê–ë–û–†–ê–¢–û–†–ù"
        r"|–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–ª—å–Ω|–ò–ù–°–¢–†–£–ú–ï–ù–¢–ê–õ–¨–ù)(.*?)"
        r"(?=\nI{1,3}V?\b|\nVII?\b|–ª–µ—á–µ–Ω–∏–µ|–õ–ï–ß–ï–ù–ò–ï|\Z)",
        re.IGNORECASE | re.DOTALL,
    ),
    "treatment": re.compile(
        r"(?:–ª–µ—á–µ–Ω–∏–µ|–õ–ï–ß–ï–ù–ò–ï|–º–µ–¥–∏–∫–∞–º–µ–Ω—Ç–æ–∑–Ω|–ú–ï–î–ò–ö–ê–ú–ï–ù–¢–û–ó–ù"
        r"|—Ö–∏—Ä—É—Ä–≥–∏—á–µ—Å–∫|–•–ò–†–£–†–ì–ò–ß–ï–°–ö)(.*?)"
        r"(?=\nV{1,3}\b|\nVII?\b|–ø—Ä–æ—Ñ–∏–ª–∞–∫—Ç–∏–∫–∞|–ü–†–û–§–ò–õ–ê–ö–¢–ò–ö–ê|\Z)",
        re.IGNORECASE | re.DOTALL,
    ),
}


def extract_protocol_name(text: str, source_file: str) -> str:
    """–†–µ–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ—Ç–æ–∫–æ–ª–∞, –Ω–µ '–û–¥–æ–±—Ä–µ–Ω'."""
    m = re.search(
        r"–ö–õ–ò–ù–ò–ß–ï–°–ö–ò–ô –ü–†–û–¢–û–ö–û–õ –î–ò–ê–ì–ù–û–°–¢–ò–ö–ò –ò –õ–ï–ß–ï–ù–ò–Ø\s+([^\n]{5,150})",
        text, re.IGNORECASE,
    )
    if m:
        return m.group(1).strip()
    return source_file.replace(".pdf", "").strip()


def extract_sections(text: str) -> dict[str, str]:
    sections = {}
    for key, pat in _SECTION_PATTERNS.items():
        m = pat.search(text)
        if m:
            sections[key] = m.group(1).strip()[:MAX_CHARS_PER_CHUNK]
    return sections


def build_chunks(record: dict) -> list[tuple[str, dict]]:
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è —á–∞–Ω–∫–∏–Ω–≥–∞:
    1. –ú–∞–ª–µ–Ω—å–∫–∏–µ —á–∞–Ω–∫–∏ (–¥–æ 1000 —Å–∏–º–≤) –¥–ª—è —á–µ—Ç–∫–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤.
    2. –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –≤–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–æ—Ç–æ–∫–æ–ª–∞ –≤ —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞.
    3. –ü—Ä–∏–≤—è–∑–∫–∞ ICD-10 –∫–æ–¥–æ–≤ –∫ –∫–∞–∂–¥–æ–º—É —á–∞–Ω–∫—É –¥–ª—è LLM.
    """
    raw_text  = record.get("text", "")
    source    = record.get("source_file", "unknown")
    protocol  = record.get("protocol_id", "")
    icd_codes = record.get("icd_codes", [])
    real_name = extract_protocol_name(raw_text, source)

    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ª–µ—Ç—è—Ç –≤ Qdrant
    base_payload = {
        "title":       real_name,
        "source_file": source,
        "protocol_id": protocol,
        "icd_codes":   icd_codes,
    }

    sections = extract_sections(raw_text)
    chunks: list[tuple[str, dict]] = []

    # –õ–∏–º–∏—Ç —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ —Ñ–æ–∫—É—Å–∞ –≤–µ–∫—Ç–æ—Ä–∞
    CHUNK_LIMIT = 1000 

    # 1. –°–µ–∫—Ü–∏—è –ö–õ–ò–ù–ò–ö–ê (—Å–∏–º–ø—Ç–æ–º—ã + –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞) - —Å–∞–º–∞—è –≤–∞–∂–Ω–∞—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –∂–∞–ª–æ–±–∞–º
    clinical_text = " ".join(filter(None, [
        sections.get("symptoms", ""),
        sections.get("diagnosis", ""),
    ])).strip()

    if clinical_text:
        # –†–∞–∑–±–∏–≤–∞–µ–º –¥–ª–∏–Ω–Ω—É—é –∫–ª–∏–Ω–∏—á–µ—Å–∫—É—é –∫–∞—Ä—Ç–∏–Ω—É –Ω–∞ —á–∞—Å—Ç–∏, –µ—Å–ª–∏ –æ–Ω–∞ –æ–≥—Ä–æ–º–Ω–∞—è
        parts = [clinical_text[i:i+CHUNK_LIMIT] for i in range(0, len(clinical_text), CHUNK_LIMIT)]
        for i, part in enumerate(parts):
            # –í —Ç–µ–∫—Å—Ç –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–æ–±–∞–≤–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ, —á—Ç–æ–±—ã –≤–µ–∫—Ç–æ—Ä "–∑–Ω–∞–ª" –æ —á–µ–º —Ä–µ—á—å
            text_for_embedding = f"–ü—Ä–æ—Ç–æ–∫–æ–ª: {real_name}. –ö–ª–∏–Ω–∏—á–µ—Å–∫–∞—è –∫–∞—Ä—Ç–∏–Ω–∞: {part}"
            
            payload = {**base_payload, "chunk_type": "clinical", "chunk_index": i, "text": part}
            chunks.append((text_for_embedding, payload))

    # 2. –°–µ–∫—Ü–∏—è –õ–ï–ß–ï–ù–ò–ï
    treatment_text = sections.get("treatment", "").strip()
    if treatment_text:
        parts = [treatment_text[i:i+CHUNK_LIMIT] for i in range(0, len(treatment_text), CHUNK_LIMIT)]
        for i, part in enumerate(parts):
            text_for_embedding = f"–ü—Ä–æ—Ç–æ–∫–æ–ª: {real_name}. –õ–µ—á–µ–Ω–∏–µ –∏ —Ç–∞–∫—Ç–∏–∫–∞: {part}"
            
            payload = {**base_payload, "chunk_type": "treatment", "chunk_index": i, "text": part}
            chunks.append((text_for_embedding, payload))

    # 3. –§–æ–ª–±–µ–∫ (–µ—Å–ª–∏ —Ä–µ–≥–µ–∫—Å—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏)
    if not chunks:
        body = raw_text[600:] # –ü—Ä–æ–ø—É—Å–∫ –±—é—Ä–æ–∫—Ä–∞—Ç–∏–∏
        step = 800
        for i, start in enumerate(range(0, min(len(body), 5000), step)):
            part = body[start : start + CHUNK_LIMIT].strip()
            if len(part) < 150: continue
            
            text_for_embedding = f"–ü—Ä–æ—Ç–æ–∫–æ–ª: {real_name}. –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: {part}"
            payload = {**base_payload, "chunk_type": "sliding", "chunk_index": i, "text": part}
            chunks.append((text_for_embedding, payload))

    return chunks

# ‚îÄ‚îÄ‚îÄ –£—Ç–∏–ª–∏—Ç—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def make_id(key: str) -> str:
    h = hashlib.md5(key.encode()).hexdigest()
    return f"{h[:8]}-{h[8:12]}-{h[12:16]}-{h[16:20]}-{h[20:32]}"


def load_jsonl(path: str) -> list[dict]:
    records = []
    with open(path, encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line:
                records.append(json.loads(line))
    print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(records)}")
    return records


# ‚îÄ‚îÄ‚îÄ –®–∞–≥ 1: —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ ‚Üí –∫—ç—à ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def encode_and_cache(records: list[dict], model: SentenceTransformer, cache_path: str) -> None:
    cached_ids: set[str] = set()
    if Path(cache_path).exists():
        with open(cache_path, encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    cached_ids.add(json.loads(line)["id"])
        print(f"–ù–∞–π–¥–µ–Ω –∫—ç—à: {len(cached_ids)} —Ç–æ—á–µ–∫, –¥–æ–∫–æ–¥–∏—Ä—É–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ...")

    all_items: list[tuple[str, str, dict]] = []
    skipped = 0

    for record in tqdm(records, desc="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —á–∞–Ω–∫–æ–≤"):
        if not record.get("text", "").strip():
            skipped += 1
            continue
        if not record.get("icd_codes"):
            skipped += 1
            continue

        source   = record.get("source_file", "unknown")
        protocol = record.get("protocol_id", "")

        for text, payload in build_chunks(record):
            id_key   = f"{source}__{protocol}__{payload['chunk_index']}__{payload['chunk_type']}"
            point_id = make_id(id_key)
            if point_id in cached_ids:
                continue
            all_items.append((text, point_id, payload))

    if skipped:
        print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ–≤–∞–ª–∏–¥–Ω—ã—Ö): {skipped}")

    if not all_items:
        print("–í—Å–µ —Ç–æ—á–∫–∏ —É–∂–µ –≤ –∫—ç—à–µ.")
        return

    print(f"\n–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ {len(all_items)} —á–∞–Ω–∫–æ–≤ (–±–∞—Ç—á={BATCH_SIZE})...")

    with open(cache_path, "a", encoding="utf-8") as cache_f:
        for start in tqdm(range(0, len(all_items), BATCH_SIZE), desc="–≠–º–±–µ–¥–¥–∏–Ω–≥–∏"):
            batch   = all_items[start : start + BATCH_SIZE]
            texts   = [item[0] for item in batch]

            # bge-m3 –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å–æ–≤ "passage:" –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–∏
            vectors = model.encode(
                texts,
                show_progress_bar=False,
                normalize_embeddings=True,
                batch_size=BATCH_SIZE,
                prompt_name="document"
            )

            for (_, point_id, payload), vector in zip(batch, vectors):
                row = {"id": point_id, "vector": vector.tolist(), "payload": payload}
                cache_f.write(json.dumps(row, ensure_ascii=False) + "\n")

    total = len(cached_ids) + len(all_items)
    print(f"‚úÖ –ö—ç—à —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {cache_path} ({total} —Ç–æ—á–µ–∫)")


# ‚îÄ‚îÄ‚îÄ –®–∞–≥ 2: –∫—ç—à ‚Üí Qdrant ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def upload_from_cache(client: QdrantClient, cache_path: str) -> None:
    if not Path(cache_path).exists():
        raise FileNotFoundError(f"–ö—ç—à –Ω–µ –Ω–∞–π–¥–µ–Ω: {cache_path}. –ó–∞–ø—É—Å—Ç–∏—Ç–µ --encode-only —Å–Ω–∞—á–∞–ª–∞.")

    print("–ü—Ä–æ–≤–µ—Ä–∫–∞ —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ç–æ—á–µ–∫ –≤ Qdrant...")
    existing_ids: set[str] = set()
    offset = None
    while True:
        result, offset = client.scroll(
            collection_name=COLLECTION_NAME,
            limit=1000, offset=offset,
            with_payload=False, with_vectors=False,
        )
        for p in result:
            existing_ids.add(str(p.id))
        if offset is None:
            break
    print(f"  –í Qdrant —É–∂–µ {len(existing_ids)} —Ç–æ—á–µ–∫.")

    with open(cache_path, encoding="utf-8") as f:
        lines = [l for l in f if l.strip()]

    batch: list[PointStruct] = []
    total_uploaded = 0
    skipped = 0

    for line in tqdm(lines, desc="–ó–∞–≥—Ä—É–∑–∫–∞ –≤ Qdrant"):
        row = json.loads(line)
        if row["id"] in existing_ids:
            skipped += 1
            continue
        batch.append(PointStruct(id=row["id"], vector=row["vector"], payload=row["payload"]))
        if len(batch) >= 256:
            client.upsert(collection_name=COLLECTION_NAME, points=batch)
            total_uploaded += len(batch)
            batch = []

    if batch:
        client.upsert(collection_name=COLLECTION_NAME, points=batch)
        total_uploaded += len(batch)

    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {total_uploaded} –Ω–æ–≤—ã—Ö —Ç–æ—á–µ–∫ (–ø—Ä–æ–ø—É—â–µ–Ω–æ: {skipped})")


# ‚îÄ‚îÄ‚îÄ –ö–æ–ª–ª–µ–∫—Ü–∏—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def create_collection(client: QdrantClient) -> None:
    existing = {c.name for c in client.get_collections().collections}
    if COLLECTION_NAME in existing:
        info = client.get_collection(COLLECTION_NAME)
        size = info.config.params.vectors.size
        if size != VECTOR_SIZE:
            print(f"  ‚ö†Ô∏è  –ü–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–µ–∫—Ü–∏—é (dim={size} ‚Üí {VECTOR_SIZE})...")
            client.delete_collection(COLLECTION_NAME)
        else:
            print(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è '{COLLECTION_NAME}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç (dim={size}).")
            return

    client.create_collection(
        collection_name=COLLECTION_NAME,
        vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE),
        optimizers_config=OptimizersConfigDiff(indexing_threshold=20_000),
    )
    print(f"–ö–æ–ª–ª–µ–∫—Ü–∏—è '{COLLECTION_NAME}' —Å–æ–∑–¥–∞–Ω–∞ (dim={VECTOR_SIZE}).")


# ‚îÄ‚îÄ‚îÄ –ü–æ–∏—Å–∫ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def demo_search(client: QdrantClient, model: SentenceTransformer, query: str) -> None:
    try:
        query_vector = model.encode(query, normalize_embeddings=True, prompt_name="query").tolist()
    except Exception:
        query_vector = model.encode(query, normalize_embeddings=True).tolist()

    results = client.query_points(
        collection_name=COLLECTION_NAME,
        query=query_vector,
        limit=5,
        with_payload=True,
    ).points  # <- .points –≤ –∫–æ–Ω—Ü–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ

    print(f"\n{'='*60}")
    print(f"–ü–æ–∏—Å–∫: '{query}'")
    print('='*60)
    for i, r in enumerate(results, 1):
        p = r.payload
        print(f"\n[{i}] score={r.score:.4f}")
        print(f"    üìã {p.get('title', p.get('source_file'))}")
        print(f"    üè• –ú–ö–ë: {', '.join(p.get('icd_codes', [])[:5])}")
        print(f"    üìÑ –¢–∏–ø: {p.get('chunk_type', '?')}")
        print(f"    üí¨ {p.get('text', '')[:200]}...")


# ‚îÄ‚îÄ‚îÄ –¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def main():
    parser = argparse.ArgumentParser(description="–ó–∞–≥—Ä—É–∑–∫–∞ –º–µ–¥–ø—Ä–æ—Ç–æ–∫–æ–ª–æ–≤ –≤ Qdrant (bge-m3 + MPS)")
    parser.add_argument("--input",       required=True,           help="–ü—É—Ç—å –∫ .jsonl —Ñ–∞–π–ª—É")
    parser.add_argument("--url",         default=QDRANT_URL,      help="URL Qdrant —Å–µ—Ä–≤–µ—Ä–∞")
    parser.add_argument("--cache",       default=CACHE_FILE,      help="–ü—É—Ç—å –∫ –∫—ç—à—É —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    parser.add_argument("--query",       default=None,            help="–¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏")
    parser.add_argument("--encode-only", action="store_true",     help="–¢–æ–ª—å–∫–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, –±–µ–∑ Qdrant")
    parser.add_argument("--upload-only", action="store_true",     help="–¢–æ–ª—å–∫–æ –∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑ –∫—ç—à–∞")
    parser.add_argument("--model",       default=EMBEDDING_MODEL, help="–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    args = parser.parse_args()

    if not Path(args.input).exists():
        raise FileNotFoundError(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.input}")

    model = None

    if not args.upload_only:
        device = get_device()
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {args.model}")
        model = SentenceTransformer(args.model, device=device)

        if device == "mps":
            print("–ü—Ä–æ–≥—Ä–µ–≤ MPS...")
            model.encode(["—Ç–µ—Å—Ç"], show_progress_bar=False)

        records = load_jsonl(args.input)
        encode_and_cache(records, model, args.cache)

    if args.encode_only:
        print(f"\n–†–µ–∂–∏–º --encode-only –∑–∞–≤–µ—Ä—à—ë–Ω. –ö—ç—à: {args.cache}")
        return

    print(f"\n–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Qdrant: {args.url}")
    qdrant_client = QdrantClient(url=args.url)
    create_collection(qdrant_client)
    upload_from_cache(qdrant_client, args.cache)
    print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –ö–æ–ª–ª–µ–∫—Ü–∏—è: {COLLECTION_NAME}")

    if model is None:
        device = get_device()
        model = SentenceTransformer(args.model, device=device)

    query = args.query or "HELLP —Å–∏–Ω–¥—Ä–æ–º –ª–µ—á–µ–Ω–∏–µ"
    demo_search(qdrant_client, model, query)


if __name__ == "__main__":
    main()